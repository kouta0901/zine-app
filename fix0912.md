問題点（小説化にZINE内容が反映されにくい主因）
バックエンド不一致
フロントは /novelize-with-images を呼ぶが、サーバにはそのエンドポイントが無い。結果的にエラー→フォールバックのテキストベース生成に流れるため、画像・OCR・空間解析の成果がモデルに届かない。
クライアント側AI解析がモックになりやすい
zine-creator.tsx は use client。lib/ocr.ts と lib/captioning.ts は「サーバ側のみ実初期化」設計のため、ブラウザ実行ではモック文章（汎用的で薄い内容）になる。仮に /novelize-with-images があっても、解析データの質が低下。
プロンプトの「分離」実装が未着火
lib/api.ts で NOVEL_SYSTEM_PROMPT や IMAGE_ANALYSIS_PROMPT を付与しているが、実サーバが受け取らないため効果ゼロ。
位置情報・近接テキストの情報損失
getElementPosition() の基準がキャンバス（1400x900）とズレ（高さ800/幅600の仮定）ており、位置記述が不正確。
近接テキストは200pxしきい値で「最も近い1つ」かつ50字にトリム。画像と複数テキストの対応関係が落ちる。
クリーニングの副作用の可能性
生成後の整形で「見出し/タイトル風の行」を広く削除。画像由来の短文やラベルが巻き添えで消える可能性。
具体的な改善提案（優先度順）
最高優先: バックエンドに画像ベース小説化エンドポイントを実装
api/server.ts に POST /novelize-with-images を追加し、入力: images[]（base64）、enhancedAnalysis[]、user_prompt/system_prompt を許容。
サーバ側で Vertex/Gemini に投げる際、contents に「テキスト部分＋画像の inlineData」を正しく積む（順序はZINEページ順）。
画像ごとに OCR/キャプションを「サーバ側で」実行する設計に変更してもよい（ブラウザモック回避）。
次善策（短期）：エンドポイントを使わず、既存 /novelize に全情報を統合した巨大プロンプトで渡す
lib/api.ts の novelizeWithImagesEnhanced を一時的に /novelize 呼び出しに変更し、user_prompt に
画像ページ順の「OCR全文」「キャプション全文」「空間解析記述」「近接テキスト」と共に
「この情報をすべて本文に反映する」厳格ルールを埋める。
画像本体は送れないが、テキスト化した解析情報を最大限使わせる。
解析のサーバ移譲
extractZineImages() で得た images[] をそのままサーバへ送り、/novelize-with-images 側で
Document AI OCR、Gemini Caption、空間解析（サーバ実行）を行い「高精度・一貫の解析」を生成し、それを使って小説化する。
これでブラウザモック問題を解消。
位置・近接情報の改善
getElementPosition() の正規化をキャンバス基準に合わせる（幅1400・高さ900で分位を判断）。
近接テキストは「複数件」を距離昇順で数件採用＋距離と方向（上/下/左/右）を明示。50字トリムをやめ、十分な長さを残す。

クリーニングの見直し
タイトルや章見出しを消す正規表現を限定化。画像内テキストやラベルが消えないよう、削除対象を「行頭に典型的な章記法がある場合」に限定。
ページごとの「順序」と「関係」の明示
detailedPrompt に「ページ順＝時系列」「画像-テキストの対応ペア」「レイアウトの関係（例: 画像の右に説明）」を体系化して列挙する。
すぐできる差分例（短期ワークアラウンド）
lib/api.ts の画像ベース呼び出しを既存の /novelize に統合（エンドポイント未実装でも動く）


// 既存関数を差し替え（アイデア）: 画像やenhancedAnalysisを user_prompt に埋め込んで /novelize を叩く
export async function novelizeWithImagesEnhanced(payload: {
  concept: string; world: string; images: string[]; title: string;
  imageDescriptions?: string[]; detailedPrompt?: string; enhancedAnalysis?: Array<any>;
}): Promise<{ text: string }> {
  const userPrompt = `
【画像ベース小説化・厳格要件】
- 以下のページ順に沿って、OCR/キャプション/空間解析/近接テキストの全要素を本文に反映すること。
- 画像内テキストはセリフ/ラベルとして必ず取り込む。略さない。
- 人物外見・表情・背景・色彩・構図の情報をすべて描写に織り込む。

【コンセプト】${payload.concept}
【世界観】${payload.world}
${payload.detailedPrompt ?? ""}

【ページ解析一覧】
${(payload.enhancedAnalysis ?? []).map((d, i)=>`[Page ${i+1}]
OCR: ${d.ocrText}
Caption: ${d.caption}
NearbyText: ${d.nearbyText}
Spatial: ${d.spatialContext}
`).join('\n')}

【補助ディスクリプション（要反映）】
${(payload.imageDescriptions ?? []).join('\n')}
  `.trim();

  return novelize({ concept: payload.concept, world: payload.world, prompt: userPrompt })
}


const relativeY = el.y / 900
const relativeX = el.x / 1400


なぜ今うまくいかないのか（要点）
画像＋解析データを使うルートが、サーバ未実装のため実質使われていない。
クライアントでのOCR/キャプションがモック挙動になりやすく、解析品質が低い。
位置・近接の表現が粗く、画像内容→物語のブリッジ情報が弱い。
出力整形の削除規則が強すぎ、せっかく拾った短文テキストを落としている可能性。
推奨アクション（段階的）
短期（今日）
novelizeWithImagesEnhanced を /novelize に統合する暫定実装で、解析情報をすべてプロンプトに含める。
位置正規化と近接テキストの複数化、クリーニング緩和。
中期（今週）
POST /novelize-with-images 実装（サーバ）。contents に画像 inlineData を並べ、text と組み合わせてGeminiに送る。
OCR/キャプション/空間解析のサーバ移譲でモックを排除。
長期
画像×テキストの対応付けをより構造化（JSONで座標・対応関係をサーバに送る→サーバで精密プロンプト生成）。
出力の自動評価（各ページの要件充足チェック）で再試行制御。
以上を踏まえ、どちらから着手しますか？すぐ直せる暫定案（API差し替え＋位置/近接/整形の微修正）なら私が具体的な編集案を用意します。